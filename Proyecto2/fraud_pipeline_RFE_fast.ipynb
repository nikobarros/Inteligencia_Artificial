{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd66e98",
   "metadata": {},
   "source": [
    "# Proyecto 2 — Pipeline con **RFE** (rápido)\n",
    "Este notebook usa **RFE** como selección de características para **todos** los clasificadores.\n",
    "- Se evita RFECV (muy costoso) y se **tunea `n_features_to_select`** vía GridSearchCV.\n",
    "- **SMOTE** va **dentro** de la CV para no fugar información.\n",
    "- **CV=3** para acelerar.\n",
    "\n",
    "**Modelos:** k-NN (RFE con LogisticRegression), Árbol (RFE con Árbol), SVM lineal (RFE con LinearSVC), Random Forest (RFE con RandomForest).\n",
    "\n",
    "**Entrada:** `creditcard.csv` en la misma carpeta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193acf1",
   "metadata": {},
   "source": [
    "## 1) Setup e importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841adc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -q scikit-learn imbalanced-learn pandas numpy\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e4083",
   "metadata": {},
   "source": [
    "## 2) Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072c1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cargar_datos(path_csv: str = \"dataset/creditcard.csv\") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_csv)\n",
    "    return df\n",
    "\n",
    "def construir_preprocesador(feature_names):\n",
    "    base_cols = [\"Time\", \"Amount\"]\n",
    "    pca_cols = [c for c in feature_names if c.startswith(\"V\")]\n",
    "    return ColumnTransformer([\n",
    "        (\"robust\", RobustScaler(), base_cols),\n",
    "        (\"std\", StandardScaler(with_mean=False), pca_cols),\n",
    "    ], remainder=\"drop\")\n",
    "\n",
    "def metricas_desde_confusion(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-12)\n",
    "    sensibilidad = tp / (tp + fn + 1e-12)\n",
    "    especificidad = tn / (tn + fp + 1e-12)\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    f1 = 2 * precision * sensibilidad / (precision + sensibilidad + 1e-12)\n",
    "    return dict(exactitud=acc, sensibilidad=sensibilidad, especificidad=especificidad,\n",
    "                precision=precision, f1=f1, tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn))\n",
    "\n",
    "def evaluar_modelo(nombre, y_true, y_pred, y_score=None):\n",
    "    print(f\"\\\\n=== {nombre} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Matriz de confusión:\\\\n\", cm)\n",
    "    resumen = metricas_desde_confusion(cm)\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            resumen[\"roc_auc\"] = auc\n",
    "            print(f\"ROC-AUC: {auc:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(\"No fue posible calcular ROC-AUC:\", e)\n",
    "    return resumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9ffc7",
   "metadata": {},
   "source": [
    "## 3) Pipelines con RFE para los 4 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline  # to avoid confusion, but we use ImbPipeline\n",
    "\n",
    "def pipeline_knn_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe_est = LogisticRegression(solver=\"liblinear\", max_iter=200, random_state=RANDOM_STATE)\n",
    "    rfe = RFE(estimator=rfe_est, n_features_to_select=20, step=2)\n",
    "    knn = KNeighborsClassifier()\n",
    "    pipe = ImbPipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "        (\"rfe\", rfe),\n",
    "        (\"clf\", knn),\n",
    "    ])\n",
    "    param_grid = {\n",
    "        \"rfe__n_features_to_select\": [16, 20],\n",
    "        \"clf__n_neighbors\": [3, 5, 7],\n",
    "        \"clf__metric\": [\"manhattan\", \"euclidean\"],\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_arbol_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "              n_features_to_select=5, step=2)\n",
    "    dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "        (\"rfe\", rfe),\n",
    "        (\"clf\", dt),\n",
    "    ])\n",
    "    param_grid = {\n",
    "        \"rfe__n_features_to_select\": [12, 16, 20, 24],\n",
    "        \"clf__max_depth\": [10, 15, None],\n",
    "        \"clf__min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_svm_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=LinearSVC(random_state=RANDOM_STATE, tol=1e-3, C=1.0),\n",
    "              n_features_to_select=20, step=2)\n",
    "    svm = LinearSVC(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "        (\"rfe\", rfe),\n",
    "        (\"clf\", svm),\n",
    "    ])\n",
    "    param_grid = {\n",
    "        \"rfe__n_features_to_select\": [12, 16, 20, 24],\n",
    "        \"clf__C\": [0.1, 1, 10],\n",
    "        \"clf__tol\": [1e-3, 1e-4],\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_rf_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
    "              n_features_to_select=20, step=2)\n",
    "    rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "        (\"rfe\", rfe),\n",
    "        (\"clf\", rf),\n",
    "    ])\n",
    "    param_grid = {\n",
    "        \"rfe__n_features_to_select\": [12, 16, 20, 24],\n",
    "        \"clf__n_estimators\": [200, 400],\n",
    "        \"clf__max_depth\": [None, 15],\n",
    "        \"clf__min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1, refit=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d15a4",
   "metadata": {},
   "source": [
    "## 4) Carga y split 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraude total: 0.1727% | Train: 0.1725% | Test: 0.1732%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Cargar datos y split ===\n",
    "path_csv = \"dataset/creditcard.csv\"\n",
    "if not os.path.exists(path_csv):\n",
    "    raise FileNotFoundError(\"No se encontró creditcard.csv (Kaggle: mlg-ulb/creditcardfraud).\")\n",
    "df = cargar_datos(path_csv)\n",
    "feature_cols = [\"Time\", \"Amount\"] + [f\"V{i}\" for i in range(1, 29)]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"Class\"].astype(int).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"Fraude total: {y.mean()*100:.4f}% | Train: {y_train.mean()*100:.4f}% | Test: {y_test.mean()*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c4662",
   "metadata": {},
   "source": [
    "## 5) Entrenamiento + evaluación + resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a6230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Entrenando k-NN (RFE+LR) ...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Entrenamiento + GridSearchCV (RFE en todos) ===\n",
    "modelos = {\n",
    "    \"k-NN (RFE+LR)\": pipeline_knn_rfe(feature_cols),\n",
    "    \"Árbol (RFE+Árbol)\": pipeline_arbol_rfe(feature_cols),\n",
    "    \"SVM lineal (RFE+LinearSVC)\": pipeline_svm_rfe(feature_cols),\n",
    "    \"Random Forest (RFE+RF)\": pipeline_rf_rfe(feature_cols),\n",
    "}\n",
    "resultados = {}\n",
    "for nombre, grid in modelos.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Entrenando {nombre} ...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Mejores hiperparámetros ({nombre}): {grid.best_params_}\")\n",
    "    best = grid.best_estimator_\n",
    "    y_pred = best.predict(X_test)\n",
    "    y_score = None\n",
    "    if hasattr(best, \"decision_function\"):\n",
    "        try:\n",
    "            y_score = best.decision_function(X_test)\n",
    "        except Exception:\n",
    "            y_score = None\n",
    "    if y_score is None and hasattr(best, \"predict_proba\"):\n",
    "        try:\n",
    "            y_score = best.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            y_score = None\n",
    "    resumen = evaluar_modelo(nombre, y_test, y_pred, y_score)\n",
    "    resultados[nombre] = {\"mejores_hiperparametros\": grid.best_params_, **resumen}\n",
    "\n",
    "import pandas as pd\n",
    "resumen_df = pd.DataFrame(resultados).T[[\"sensibilidad\",\"especificidad\",\"precision\",\"f1\",\"exactitud\"]]\\\n",
    "             .sort_values(by=\"sensibilidad\", ascending=False)\n",
    "display(resumen_df)\n",
    "resumen_df.to_csv(\"resumen_metricas_modelos_RFE.csv\", index=True)\n",
    "print(\"CSV guardado: resumen_metricas_modelos_RFE.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d61f3",
   "metadata": {},
   "source": [
    "## 6) (Opcional) Ajuste de umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === (Opcional) Ajuste de umbral sobre el modelo con mayor sensibilidad ===\n",
    "mejor_modelo = resumen_df.index[0]\n",
    "best_estimator = modelos[mejor_modelo].best_estimator_\n",
    "if hasattr(best_estimator, \"predict_proba\") or hasattr(best_estimator, \"decision_function\"):\n",
    "    scores = best_estimator.predict_proba(X_test)[:, 1] if hasattr(best_estimator, \"predict_proba\") else best_estimator.decision_function(X_test)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, scores)\n",
    "    objetivo = 0.90\n",
    "    import numpy as np\n",
    "    idx = np.where(recalls >= objetivo)[0]\n",
    "    if len(idx) > 0:\n",
    "        thr = thresholds[idx[0]-1] if idx[0] > 0 else thresholds[0]\n",
    "        y_pred_thr = (scores >= thr).astype(int)\n",
    "        print(f\"Ajuste de umbral a ~{objetivo:.2f} de sensibilidad — threshold={thr:.4f}\")\n",
    "        _ = evaluar_modelo(f\"{mejor_modelo} (umbral ajustado)\", y_test, y_pred_thr, scores)\n",
    "    else:\n",
    "        print(\"No se logró alcanzar la sensibilidad objetivo en test.\")\n",
    "else:\n",
    "    print(\"El mejor modelo no entrega score continuo; se omite ajuste de umbral.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
