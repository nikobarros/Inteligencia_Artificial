{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c47eb4f",
   "metadata": {},
   "source": [
    "# Proyecto 2 — Pipeline con **RFE** (ULTRA-RÁPIDO)\n",
    "Versión optimizada para tiempo: **CV=2**, grids mínimos, y RFE con pocas opciones.\n",
    "- Mantiene **RFE** en los 4 modelos y **SMOTE dentro de CV** (sin fugas).\n",
    "- Pensado para terminar en ~10–15 minutos en CPU promedio.\n",
    "\n",
    "**Entrada:** `creditcard.csv` en la misma carpeta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993295bb",
   "metadata": {},
   "source": [
    "## 1) Setup e importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65265f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q scikit-learn imbalanced-learn pandas numpy\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e968f2",
   "metadata": {},
   "source": [
    "## 2) Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe6756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(path_csv: str = \"dataset/creditcard.csv\") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_csv)\n",
    "    return df\n",
    "\n",
    "def construir_preprocesador(feature_names):\n",
    "    base_cols = [\"Time\", \"Amount\"]\n",
    "    pca_cols = [c for c in feature_names if c.startswith(\"V\")]\n",
    "    return ColumnTransformer([\n",
    "    ('all_features', StandardScaler(), base_cols + pca_cols)\n",
    "])\n",
    "\n",
    "def metricas_desde_confusion(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-12)\n",
    "    sensibilidad = tp / (tp + fn + 1e-12)\n",
    "    especificidad = tn / (tn + fp + 1e-12)\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    f1 = 2 * precision * sensibilidad / (precision + sensibilidad + 1e-12)\n",
    "    return dict(exactitud=acc, sensibilidad=sensibilidad, especificidad=especificidad,\n",
    "                precision=precision, f1=f1, tp=int(tp), fp=int(fp), tn=int(tn), fn=int(fn))\n",
    "\n",
    "def evaluar_modelo(nombre, y_true, y_pred, y_score=None):\n",
    "    print(f\"\\n=== {nombre} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Matriz de confusión:\\n\", cm)\n",
    "    resumen = metricas_desde_confusion(cm)\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            resumen[\"roc_auc\"] = auc\n",
    "            print(f\"ROC-AUC: {auc:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(\"No fue posible calcular ROC-AUC:\", e)\n",
    "    return resumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effef0d3",
   "metadata": {},
   "source": [
    "## 3) Pipelines con RFE (grids mínimos, CV=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b284fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipeline_knn_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe_est = LogisticRegression(solver=\"liblinear\", max_iter=200, random_state=RANDOM_STATE)\n",
    "    rfe = RFE(estimator=rfe_est, n_features_to_select=3, step=5)  # step aumentado a 5 (antes 2) para acelerar RFE\n",
    "    knn = KNeighborsClassifier()\n",
    "    pipe = ImbPipeline([(\"pre\", pre), (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)), (\"rfe\", rfe), (\"clf\", knn)])\n",
    "    param_dist = {\"rfe__n_features_to_select\": [10, 15], \"clf__n_neighbors\": [5], \"clf__metric\": [\"euclidean\"]}\n",
    "    # Modificación: usar RandomizedSearchCV (n_iter=2) en lugar de GridSearchCV para acelerar la búsqueda\n",
    "    # Espacio de búsqueda reducido: métrica solo 'euclidean' (antes también 'manhattan'),\n",
    "    # n_neighbors fijo en 5, y RFE probando con 10 o 15 características en lugar de 16 o 20.\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return RandomizedSearchCV(pipe, param_dist, scoring=\"f1\", cv=cv, n_iter=2, random_state=RANDOM_STATE, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_arbol_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=15), n_features_to_select=3, step=5)  # step aumentado a 5 (antes 2), limitamos profundidad a 15 en RFE\n",
    "    dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([(\"pre\", pre), (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)), (\"rfe\", rfe), (\"clf\", dt)])\n",
    "    param_dist = {\"rfe__n_features_to_select\": [10, 15], \"clf__max_depth\": [15], \"clf__min_samples_leaf\": [1, 2]}\n",
    "    # Modificación: usar RandomizedSearchCV (n_iter=3) en lugar de GridSearchCV para acelerar la búsqueda\n",
    "    # Espacio de búsqueda reducido: max_depth fijo en 15 (antes se probaba None y 15),\n",
    "    # y RFE probando con 10 o 15 características en lugar de 16 o 20 (min_samples_leaf sigue en [1,2]).\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return RandomizedSearchCV(pipe, param_dist, scoring=\"f1\", cv=cv, n_iter=3, random_state=RANDOM_STATE, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_svm_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=LinearSVC(random_state=RANDOM_STATE, tol=1e-3, C=1.0), n_features_to_select=3, step=5)  # step aumentado a 5 (antes 2) para acelerar RFE\n",
    "    svm = LinearSVC(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([(\"pre\", pre), (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)), (\"rfe\", rfe), (\"clf\", svm)])\n",
    "    param_dist = {\"rfe__n_features_to_select\": [10, 15], \"clf__C\": [1], \"clf__tol\": [1e-3]}\n",
    "    # Modificación: usar RandomizedSearchCV (n_iter=2) en lugar de GridSearchCV para acelerar la búsqueda\n",
    "    # Espacio de búsqueda reducido: tol fijo en 1e-3 (antes 1e-3 y 1e-4),\n",
    "    # y RFE probando con 10 o 15 características en lugar de 16 o 20 (C=1 fijo).\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return RandomizedSearchCV(pipe, param_dist, scoring=\"f1\", cv=cv, n_iter=2, random_state=RANDOM_STATE, n_jobs=-1, refit=True, verbose=1)\n",
    "\n",
    "def pipeline_rf_rfe(feature_names):\n",
    "    pre = construir_preprocesador(feature_names)\n",
    "    rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=15, random_state=RANDOM_STATE), n_features_to_select=3, step=5)  # step aumentado a 5, n_estimators=100 y max_depth=15 en RFE\n",
    "    rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "    pipe = ImbPipeline([(\"pre\", pre), (\"smote\", SMOTE(random_state=RANDOM_STATE, k_neighbors=5)), (\"rfe\", rfe), (\"clf\", rf)])\n",
    "    param_dist = {\"rfe__n_features_to_select\": [10, 15], \"clf__n_estimators\": [100], \"clf__max_depth\": [15], \"clf__min_samples_leaf\": [1, 2]}\n",
    "    # Modificación: usar RandomizedSearchCV (n_iter=3) en lugar de GridSearchCV para acelerar la búsqueda\n",
    "    # Espacio de búsqueda reducido: n_estimators fijo en 100 (antes 200) y max_depth fijo en 15 (antes None y 15),\n",
    "    # y RFE probando con 10 o 15 características en lugar de 16 o 20 (min_samples_leaf sigue [1,2]).\n",
    "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return RandomizedSearchCV(pipe, param_dist, scoring=\"f1\", cv=cv, n_iter=3, random_state=RANDOM_STATE, n_jobs=-1, refit=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7a253",
   "metadata": {},
   "source": [
    "## 4) Carga y split 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9f682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del dataset:\n",
      "Dimensiones: (284807, 31)\n",
      "Distribución de clases: Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n",
      "Porcentaje de fraude: 0.1727%\n",
      "Valores nulos: 0\n",
      "Fraude total: 0.1727% | Train: 0.1725% | Test: 0.1732%\n"
     ]
    }
   ],
   "source": [
    "# === Cargar datos y split ===\n",
    "path_csv = \"dataset/creditcard.csv\"\n",
    "if not os.path.exists(path_csv):\n",
    "    raise FileNotFoundError(\"No se encontró creditcard.csv (Kaggle: mlg-ulb/creditcardfraud).\")\n",
    "df = cargar_datos(path_csv)\n",
    "feature_cols = [\"Time\", \"Amount\"] + [f\"V{i}\" for i in range(1, 29)]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"Class\"].astype(int).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE)\n",
    "print(\"Información del dataset:\")\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"Distribución de clases: {df['Class'].value_counts()}\")\n",
    "print(f\"Porcentaje de fraude: {df['Class'].mean()*100:.4f}%\")\n",
    "print(f\"Valores nulos: {df.isnull().sum().sum()}\")\n",
    "print(f\"Fraude total: {y.mean()*100:.4f}% | Train: {y_train.mean()*100:.4f}% | Test: {y_test.mean()*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954219bb",
   "metadata": {},
   "source": [
    "## 5) Entrenamiento + evaluación + resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2cfdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas(grid, nombre_modelo):\n",
    "    best_pipe = grid.best_estimator_\n",
    "    rfe = best_pipe.named_steps[\"rfe\"]\n",
    "    seleccionadas = [f for f, s in zip(feature_cols, rfe.support_) if s]\n",
    "    print(f\"\\n {nombre_modelo}:\")\n",
    "    print(\"Características seleccionadas:\", seleccionadas)\n",
    "    return seleccionadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974d853",
   "metadata": {},
   "source": [
    "\n",
    "# *Tuning con muestra + RFE acelerado*\n",
    "- Usa **una muestra estratificada** del *train* para **tuning** (RFE + hiperparámetros).\n",
    "- Usa **RandomizedSearchCV** con **grids compactos**.\n",
    "- **RFE con `step=5`**\n",
    "- **SMOTE dentro de CV**\n",
    "- Luego **reentrena el mejor pipeline** con **todo el conjunto de entrenamiento** y evalúa en *test*.\n",
    "\n",
    "> Si ya tienes definidas variables como `X_train, X_test, y_train, y_test`, y `feature_cols`, esta sección las reutiliza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16b404ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Configuración de tuning reducido\n",
    "# =========================\n",
    "FRACTION_TUNING = 0.20     # 20% del train para búsquedas\n",
    "RANDOM_STATE    = 42\n",
    "CV_FOLDS        = 2        # rápido y razonable\n",
    "\n",
    "import numpy as np, pandas as pd, os\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utilidades\n",
    "def construir_preprocesador(feature_names):\n",
    "    base_cols = [\"Time\", \"Amount\"]\n",
    "    pca_cols  = [c for c in feature_names if c.startswith(\"V\")]\n",
    "    return ColumnTransformer([('num', StandardScaler(), base_cols + pca_cols)])\n",
    "\n",
    "def stratified_sample_for_tuning(X, y, fraction=FRACTION_TUNING, random_state=RANDOM_STATE):\n",
    "    X_tune, _, y_tune, _ = train_test_split(\n",
    "        X, y, train_size=fraction, stratify=y, random_state=random_state\n",
    "    )\n",
    "    print(f\"[TUNING] Usando {len(y_tune)} ejemplos ({fraction*100:.0f}%) para búsqueda\")\n",
    "    return X_tune, y_tune\n",
    "\n",
    "def evaluar_modelo(nombre, y_true, y_pred, y_score=None):\n",
    "    print(f\"\\n=== {nombre} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Matriz de confusión:\\n\", cm)\n",
    "    try:\n",
    "        if y_score is not None:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            print(f\"ROC-AUC: {auc:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89aa65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNING] Usando 39872 ejemplos (20%) para búsqueda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Comprobación de prerequisitos\n",
    "# =========================\n",
    "# Se espera que ya existan: X_train, X_test, y_train, y_test, feature_cols\n",
    "# Si no existen, intentamos construirlos desde creditcard.csv en ./dataset\n",
    "need_data = any(v not in globals() for v in [\"X_train\",\"X_test\",\"y_train\",\"y_test\",\"feature_cols\"])\n",
    "\n",
    "if need_data:\n",
    "    print(\"[INFO] No se detectaron variables previas: creando split 70/30 desde dataset/creditcard.csv\")\n",
    "    import pandas as pd\n",
    "    path_csv = \"dataset/creditcard.csv\"\n",
    "    if not os.path.exists(path_csv):\n",
    "        raise FileNotFoundError(\"No se encontró dataset/creditcard.csv. Descárgalo de Kaggle: mlg-ulb/creditcardfraud\")\n",
    "    df = pd.read_csv(path_csv)\n",
    "    feature_cols = [\"Time\", \"Amount\"] + [f\"V{i}\" for i in range(1, 29)]\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Class\"].astype(int).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(\"Split creado. Fraude en train/test:\", y_train.mean(), y_test.mean())\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "pre = construir_preprocesador(feature_cols)\n",
    "X_tune, y_tune = stratified_sample_for_tuning(X_train, y_train, FRACTION_TUNING, RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc1740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "\n",
      "Mejores params (tuning reducido):\n",
      "kNN: {'rfe__n_features_to_select': 12, 'clf__n_neighbors': 5, 'clf__metric': 'euclidean'}\n",
      "DT : {'rfe__n_features_to_select': 12, 'clf__min_samples_leaf': 1, 'clf__max_depth': 15}\n",
      "SVM: {'rfe__n_features_to_select': 15, 'clf__tol': 0.001, 'clf__C': 1.0}\n",
      "RF : {'rfe__n_features_to_select': 20, 'clf__n_estimators': 100, 'clf__min_samples_leaf': 2, 'clf__max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BÚSQUEDAS sobre la muestra (RFE + RandomizedSearchCV)\n",
    "# =========================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- k-NN ---\n",
    "rfe_knn = RFE(\n",
    "    estimator=LogisticRegression(solver=\"liblinear\", max_iter=200, random_state=RANDOM_STATE),\n",
    "    n_features_to_select=12, step=5\n",
    ")\n",
    "pipe_knn = ImbPipeline([\n",
    "    (\"pre\",  pre),\n",
    "    (\"sm\",   SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "    (\"rfe\",  rfe_knn),\n",
    "    (\"clf\",  KNeighborsClassifier())\n",
    "])\n",
    "dist_knn = {\n",
    "    \"rfe__n_features_to_select\": [10, 12, 15],\n",
    "    \"clf__n_neighbors\": [5, 7],\n",
    "    \"clf__metric\": [\"euclidean\"]\n",
    "}\n",
    "search_knn = RandomizedSearchCV(\n",
    "    pipe_knn, dist_knn, n_iter=4, cv=cv, scoring=\"f1\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE, refit=True, verbose=1\n",
    ")\n",
    "search_knn.fit(X_tune, y_tune)\n",
    "\n",
    "# --- Árbol ---\n",
    "rfe_dt = RFE(\n",
    "    estimator=DecisionTreeClassifier(max_depth=15, random_state=RANDOM_STATE),\n",
    "    n_features_to_select=12, step=5\n",
    ")\n",
    "pipe_dt = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"sm\",  SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "    (\"rfe\", rfe_dt),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "dist_dt = {\n",
    "    \"rfe__n_features_to_select\": [10, 12, 15],\n",
    "    \"clf__max_depth\": [15],\n",
    "    \"clf__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "search_dt = RandomizedSearchCV(\n",
    "    pipe_dt, dist_dt, n_iter=4, cv=cv, scoring=\"f1\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE, refit=True, verbose=1\n",
    ")\n",
    "search_dt.fit(X_tune, y_tune)\n",
    "\n",
    "# --- SVM lineal ---\n",
    "rfe_svm = RFE(\n",
    "    estimator=LinearSVC(random_state=RANDOM_STATE, tol=1e-3, C=1.0),\n",
    "    n_features_to_select=12, step=5\n",
    ")\n",
    "pipe_svm = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"sm\",  SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "    (\"rfe\", rfe_svm),\n",
    "    (\"clf\", LinearSVC(random_state=RANDOM_STATE))\n",
    "])\n",
    "dist_svm = {\n",
    "    \"rfe__n_features_to_select\": [10, 12, 15],\n",
    "    \"clf__C\": [0.5, 1.0],\n",
    "    \"clf__tol\": [1e-3]\n",
    "}\n",
    "search_svm = RandomizedSearchCV(\n",
    "    pipe_svm, dist_svm, n_iter=3, cv=cv, scoring=\"f1\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE, refit=True, verbose=1\n",
    ")\n",
    "search_svm.fit(X_tune, y_tune)\n",
    "\n",
    "# --- Random Forest ---\n",
    "rfe_rf = RFE(\n",
    "    estimator=RandomForestClassifier(n_estimators=100, max_depth=15, random_state=RANDOM_STATE),\n",
    "    n_features_to_select=12, step=5\n",
    ")\n",
    "pipe_rf = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"sm\",  SMOTE(random_state=RANDOM_STATE, k_neighbors=5)),\n",
    "    (\"rfe\", rfe_rf),\n",
    "    (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "dist_rf = {\n",
    "    \"rfe__n_features_to_select\": [15,18,20],\n",
    "    \"clf__n_estimators\": [100],\n",
    "    \"clf__max_depth\": [15],\n",
    "    \"clf__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "search_rf = RandomizedSearchCV(\n",
    "    pipe_rf, dist_rf, n_iter=4, cv=cv, scoring=\"f1\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE, refit=True, verbose=1\n",
    ")\n",
    "search_rf.fit(X_tune, y_tune)\n",
    "\n",
    "print(\"\\nMejores params (tuning reducido):\")\n",
    "print(\"kNN:\", search_knn.best_params_)\n",
    "print(\"DT :\", search_dt.best_params_)\n",
    "print(\"SVM:\", search_svm.best_params_)\n",
    "print(\"RF :\", search_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffa5bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " k-NN:\n",
      "Características seleccionadas: ['V1', 'V3', 'V4', 'V7', 'V8', 'V9', 'V13', 'V14', 'V15', 'V18', 'V19', 'V28']\n",
      "\n",
      " Árbol:\n",
      "Características seleccionadas: ['V1', 'V3', 'V4', 'V7', 'V14', 'V15', 'V16', 'V18', 'V19', 'V23', 'V26', 'V28']\n",
      "\n",
      " SVM lineal:\n",
      "Características seleccionadas: ['V1', 'V2', 'V3', 'V4', 'V7', 'V8', 'V9', 'V13', 'V14', 'V15', 'V16', 'V18', 'V19', 'V20', 'V26']\n",
      "\n",
      " Random Forest:\n",
      "Características seleccionadas: ['V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V21', 'V26', 'V28']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 12, 15, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejores_caracteristicas = {\n",
    "    \"kNN\": extraer_caracteristicas(search_knn, \"k-NN\"),\n",
    "    \"DT\":  extraer_caracteristicas(search_dt, \"Árbol\"),\n",
    "    \"SVM\": extraer_caracteristicas(search_svm, \"SVM lineal\"),\n",
    "    \"RF\":  extraer_caracteristicas(search_rf, \"Random Forest\"),\n",
    "}\n",
    "len(mejores_caracteristicas[\"kNN\"]), len(mejores_caracteristicas[\"DT\"]), len(mejores_caracteristicas[\"SVM\"]), len(mejores_caracteristicas[\"RF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d5ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== kNN (final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9977    0.9987     85295\n",
      "           1     0.3820    0.8311    0.5234       148\n",
      "\n",
      "    accuracy                         0.9974     85443\n",
      "   macro avg     0.6908    0.9144    0.7610     85443\n",
      "weighted avg     0.9986    0.9974    0.9979     85443\n",
      "\n",
      "Matriz de confusión:\n",
      " [[85096   199]\n",
      " [   25   123]]\n",
      "ROC-AUC: 0.9216\n",
      "\n",
      "=== Árbol (final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9946    0.9971     85295\n",
      "           1     0.1983    0.7703    0.3154       148\n",
      "\n",
      "    accuracy                         0.9942     85443\n",
      "   macro avg     0.5989    0.8824    0.6562     85443\n",
      "weighted avg     0.9982    0.9942    0.9959     85443\n",
      "\n",
      "Matriz de confusión:\n",
      " [[84834   461]\n",
      " [   34   114]]\n",
      "ROC-AUC: 0.8841\n",
      "\n",
      "=== SVM lineal (final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9808    0.9902     85295\n",
      "           1     0.0725    0.8649    0.1338       148\n",
      "\n",
      "    accuracy                         0.9806     85443\n",
      "   macro avg     0.5361    0.9228    0.5620     85443\n",
      "weighted avg     0.9982    0.9806    0.9887     85443\n",
      "\n",
      "Matriz de confusión:\n",
      " [[83658  1637]\n",
      " [   20   128]]\n",
      "ROC-AUC: 0.9701\n",
      "\n",
      "=== Random Forest (final) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9993    0.9995     85295\n",
      "           1     0.6519    0.7973    0.7173       148\n",
      "\n",
      "    accuracy                         0.9989     85443\n",
      "   macro avg     0.8258    0.8983    0.8584     85443\n",
      "weighted avg     0.9990    0.9989    0.9990     85443\n",
      "\n",
      "Matriz de confusión:\n",
      " [[85232    63]\n",
      " [   30   118]]\n",
      "ROC-AUC: 0.9572\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Reentrenar cada mejor pipeline con TODO el train\n",
    "# =========================\n",
    "best_knn = search_knn.best_estimator_.set_params(**search_knn.best_params_)\n",
    "best_dt  = search_dt.best_estimator_.set_params(**search_dt.best_params_)\n",
    "best_svm = search_svm.best_estimator_.set_params(**search_svm.best_params_)\n",
    "best_rf  = search_rf.best_estimator_.set_params(**search_rf.best_params_)\n",
    "\n",
    "best_knn.fit(X_train, y_train)\n",
    "best_dt.fit(X_train, y_train)\n",
    "best_svm.fit(X_train, y_train)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "def preds_scores(model, X):\n",
    "    try:\n",
    "        s = model.predict_proba(X)[:, 1]\n",
    "    except Exception:\n",
    "        s = model.decision_function(X)\n",
    "    return model.predict(X), s\n",
    "\n",
    "y_pred_knn, s_knn = preds_scores(best_knn, X_test)\n",
    "y_pred_dt,  s_dt  = preds_scores(best_dt,  X_test)\n",
    "y_pred_svm, s_svm = preds_scores(best_svm, X_test)\n",
    "y_pred_rf,  s_rf  = preds_scores(best_rf,  X_test)\n",
    "\n",
    "# =========================\n",
    "# Evaluar en test\n",
    "# =========================\n",
    "\n",
    "evaluar_modelo(\"kNN (final)\", y_test, y_pred_knn, s_knn)\n",
    "evaluar_modelo(\"Árbol (final)\", y_test, y_pred_dt, s_dt)\n",
    "evaluar_modelo(\"SVM lineal (final)\", y_test, y_pred_svm, s_svm)\n",
    "evaluar_modelo(\"Random Forest (final)\", y_test, y_pred_rf, s_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aba9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split listo -> X_train: (227845, 30), X_test: (56962, 30) y_train: (227845,), y_test: (56962,)\n",
      "🔧 Usando preprocesador: preprocesador\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No se encontró un pipeline/modelo con métodos .fit/.predict. Crea una variable como 'pipe_optimo' o 'clf' antes de ejecutar este bloque.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modelo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo se encontró un pipeline/modelo con métodos .fit/.predict. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCrea una variable como \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpipe_optimo\u001b[39m\u001b[33m'\u001b[39m\u001b[33m o \u001b[39m\u001b[33m'\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m antes de ejecutar este bloque.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# ---- 5) Entrenar y evaluar\u001b[39;00m\n\u001b[32m     70\u001b[39m _ = modelo.fit(X_train_prep, y_train)\n",
      "\u001b[31mRuntimeError\u001b[39m: No se encontró un pipeline/modelo con métodos .fit/.predict. Crea una variable como 'pipe_optimo' o 'clf' antes de ejecutar este bloque."
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# Split Train/Test + Evaluación\n",
    "# ===============================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ---- 1) Chequeos básicos\n",
    "if 'X' not in globals() or 'y' not in globals():\n",
    "    raise RuntimeError(\"No se encuentran las variables 'X' y 'y' en el entorno. Define X (features) e y (etiquetas) antes de ejecutar este bloque.\")\n",
    "\n",
    "# ---- 2) Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"✅ Split listo ->\",\n",
    "      f\"X_train: {getattr(X_train, 'shape', None)}, X_test: {getattr(X_test, 'shape', None)}\",\n",
    "      f\"y_train: {np.shape(y_train)}, y_test: {np.shape(y_test)}\")\n",
    "\n",
    "# ---- 3) Preparar preprocesamiento (si existe)\n",
    "preprocesador = None\n",
    "for name in ['preprocesador_optimo', 'preprocesador', 'preprocess', 'preprocessor']:\n",
    "    if name in globals():\n",
    "        preprocesador = globals()[name]\n",
    "        print(f\"🔧 Usando preprocesador: {name}\")\n",
    "        break\n",
    "\n",
    "def _to_dense(Xm):\n",
    "    try:\n",
    "        import scipy.sparse as sp\n",
    "        return Xm.toarray() if sp.issparse(Xm) else Xm\n",
    "    except Exception:\n",
    "        return Xm\n",
    "\n",
    "if preprocesador is not None:\n",
    "    X_train_prep = preprocesador.fit_transform(X_train)\n",
    "    X_test_prep  = preprocesador.transform(X_test)\n",
    "else:\n",
    "    # Si no hay preprocesador, se usan los datos tal cual\n",
    "    X_train_prep, X_test_prep = X_train, X_test\n",
    "\n",
    "# ---- 4) Detectar pipeline o modelo\n",
    "modelo = None\n",
    "pipeline_candidates = ['pipe_optimo', 'pipeline_optimo', 'pipe', 'pipeline', 'modelo_optimo']\n",
    "for name in pipeline_candidates:\n",
    "    if name in globals():\n",
    "        obj = globals()[name]\n",
    "        if hasattr(obj, 'fit') and hasattr(obj, 'predict'):\n",
    "            modelo = obj\n",
    "            print(f\"🤖 Usando pipeline/modelo: {name}\")\n",
    "            break\n",
    "\n",
    "if modelo is None:\n",
    "    model_candidates = ['k-NN', 'dt', 'rf', 'svm', 'knn', 'clf', 'modelo', 'clasificador']\n",
    "    for name in model_candidates:\n",
    "        if name in globals():\n",
    "            obj = globals()[name]\n",
    "            if hasattr(obj, 'fit') and hasattr(obj, 'predict'):\n",
    "                modelo = obj\n",
    "                print(f\"🤖 Usando modelo: {name}\")\n",
    "                break\n",
    "\n",
    "if modelo is None:\n",
    "    raise RuntimeError(\n",
    "        \"No se encontró un pipeline/modelo con métodos .fit/.predict. \"\n",
    "        \"Crea una variable como 'pipe_optimo' o 'clf' antes de ejecutar este bloque.\"\n",
    "    )\n",
    "\n",
    "# ---- 5) Entrenar y evaluar\n",
    "_ = modelo.fit(X_train_prep, y_train)\n",
    "y_pred_tr = modelo.predict(X_train_prep)\n",
    "y_pred_te = modelo.predict(X_test_prep)\n",
    "\n",
    "def _maybe_proba(estimator, Xmat):\n",
    "    # Intenta usar predict_proba o decision_function si existen\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        try:\n",
    "            proba = estimator.predict_proba(Xmat)\n",
    "            if proba is not None and proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "                return proba[:, 1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if hasattr(estimator, \"decision_function\"):\n",
    "        try:\n",
    "            scores = estimator.decision_function(Xmat)\n",
    "            # convertir a [0,1] si es necesario, pero aquí dejamos raw scores para AUC\n",
    "            return scores\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "y_score_tr = _maybe_proba(modelo, X_train_prep)\n",
    "y_score_te = _maybe_proba(modelo, X_test_prep)\n",
    "\n",
    "def _metrics(y_true, y_pred, y_score=None, title=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"binary\" if len(np.unique(y_true))==2 else \"macro\")\n",
    "    print(f\"\\n== {title} ==\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    if y_score is not None and len(np.unique(y_true))==2:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            print(f\"ROC AUC: {auc:.4f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "_metrics(y_train, y_pred_tr, y_score_tr, title=\"Métricas en TRAIN\")\n",
    "_metrics(y_test,  y_pred_te, y_score_te, title=\"Métricas en TEST\")\n",
    "\n",
    "print(\"\\n✨ Listo. Conjunto de test agregado y evaluado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
